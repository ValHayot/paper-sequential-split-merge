Title: Sequential algorithms to split and merge large 3D images

Introduction
------------

Three-dimensional images that cannot be loaded in memory are
increasingly found in a variety of disciplines. BigBrain, for
instance, is a 3D image of the human brain that represents 1~TB of
data in 3600 planes at full resolution (1$\mu$m in-plane), and 76~GB
at a 40$\mu$m isotropic resolution commonly used in neurosciences. As
such images would typically be processed on a computing cluster,
possibly using locality-aware file systems such as HDFS, software
libraries are needed to split and merged such images efficiently.  We
introduce and compare a family of algorithms to address this problem
in a sequential environment made of a single storage disk.

A disk is modeled by its read and write rates ($\alpha_r$ and
$\alpha_w$, in MB/s), and its seek speed ($\beta$, also in
MB/s).

Images are split into chunks representing 3D blocks or 2D
slices. We assume that a block or slice fits in memory. The BigBrain
would perhaps be split into 125 chunks of 600~MB. The decision to
split an image into slices or blocks, and the size of the resulting
chunks, may be constrained by the application. Some applications, for
instance spatial filtering, would commonly require blocks while other
ones such as acquisition artifact removal would rather work on
slices. Applications processing voxels individually, for instance
histogram computation or k-means clustering, could work on either
slices or blocks.

Related work: nothing found on this exact problem. Lot of things on
parallel image processing but assumes that images fit in memory or do
not consider split-and-merge as an issue.

We consider a file format where voxels are written after
alphanumerical sorting of their (x,y,z) representation. That is, x
slices are written one after the other, and in every x slice, y rows
are written one after the other. Nifti [ref] is such a format, used by
90\% of the neuroimaging community. In the remainder, we use the term
"slice" for "x slice".

Algorithms
----------

In our context, split and merge relate to the same dual problem. Without
loss of generality, we focus here on merging for the sake of
concision. Our goal then is to merge a set of $N$ chunks into a single
reconstructed 3D image with $F$ voxels of size $u$. We assume that all
chunks are of identical size and that slices are squares and blocks
are cubes. 

  Slices vs blocks
  ----------------

Algorithm 1 is the naive solution to merge slices and blocks. 

Algo 1.a: naive merging of slices.
     * For each slice
       - read slice
       - write slice in reconstructed image

Algo 1.b: naive merging of blocks.
     * For each block
       - read block
       - write block in reconstructed image

Although simplistic, algorithms 1.a. and 1.b. actually exhibit very
different performance due to the amount of seeks performed by
Algorithm 1.b. The number of seeks performed in Algo 1.b for each
block is indeed the number of rows in a slice, that is,
$(F/N)^{1/3}$. The total number of seeks, considering $N$ blocks is
thus N^{2/3}.F^{1/3}. In practice, this could lead to a tremendous
slowdown.

  Sorted blocks
  -------------

Sorting blocks alphanumerically by (x,y,z) as shown on Algorithm 2 reduces the seek time
between blocks.

Algo 2: reconstruction from sorted blocks.
* Sort blocks according to (x,y,z)
* For each block in sorted list:
  - read block
  - write block in reconstructed image

The number of seeks is not reduced but the length of seeks is.

  Cluster reads
  -------------

We fill memory with as many blocks as possible to write in slices. A
given block is read only once. Slices may be written partially.

Algo 3: reconstruction from blocks using cluster reads and sorted blocks
* Sort blocks according to (x,y,z)
* While there are unread blocks:
  * While there is memory available:
    - read next block in sorted list
  * (Partially) reconstruct slices from blocks (in memory).
  * For each partially reconstructed slice:
    - Write in reconstructed image

  Multiple reads
  --------------

We fill memory with as many voxels as possible to write groups of
entire slices. A given block may be read multiple times.

Algo 4: reconstruction from blocks using multiple reads
* While there are empty slices in the reconstructed image:
  * Determine $n$ so that $n$ slices fit in memory
  * Reconstruct next slices $s_1$, ...,  $s_n$:
     - list the blocks contributing to $s_1$, $s_n$
     - for each block in the list, read the voxels of interest
     - reconstruct $s_1$, ..., $s_n$ in memory
     - write $s_1$, ..., $s_n$

  Benchmark
  ---------

As a running example to benchmark our algorithms, we use the 76~GB
BigBrain image split in 125 chunks. The data is available [here]. We
perform experiments using the two disks below:
- HD ...
- SSD ...
Our algorithms are implemented in Python using the Nibabel library. 

Results
-------

-- Splitting, 125 blocks, HD vs SSD.
Figure 1: on HD, 5 algorithms, 10 repetitions per algorithm.
Figure 2: on SSD, 5 algorithms, 10 repetitions per algorithm. 

-- Merging, 125 blocks, HD vs SSD.
Figure 3: on HD, 5 algorithms, 10 repetitions per algorithm.
Figure 4: on SSD, 5 algorithms, 10 repetitions per algorithm. 

-- HD, 250 blocks vs 125 blocks. 
Figure 5: different block sizes

-- HD, 125 blocks, F vs F/2
- Figure 6: Different images sizes, with constant memory.

Discussion
----------

What is the best algorithm (between 3 and 4), in which context. Did we
solve the seek problem or is there any issue remaining?

Conclusion
----------

Future work:
- obliques
- parallel algorithms
- re-splitting
- model

Other datasets: talk to Francoise Peyrin, ndstore.
