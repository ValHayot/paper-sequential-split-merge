\documentclass[10pt, conference, compsocconf]{IEEEtran}

% packages
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{amsfonts} % for R symbol (the set of real numbers)
\usepackage{color}
\usepackage[pdftex]{graphicx}
\usepackage{hyperref}
\usepackage{mathtools}
\usepackage{multirow}
\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}
\DeclarePairedDelimiter{\floor}{\lfloor}{\rfloor}

% new commands
\newcommand{\todo}[1]{
  \color{red}\emph{[#1]}
  \color{black}
}

\newcommand{\note}[1]{
  \color{blue}\emph{[Note: #1]}
  \color{black}
}


\begin{document}

\title{Sequential algorithms to split and merge terabyte-scale 3D images}

\maketitle

\begin{abstract}
  \todo{The abstract goes here. DO NOT USE SPECIAL CHARACTERS,
    SYMBOLS, OR MATH IN YOUR TITLE OR ABSTRACT.}  Authors (final order
  to be discussed when the paper shapes up): Yongping/Val\'erie,
  Yuhong Yan, Tristan. Perhaps Pierre Bellec. \note{This is a note.}
\end{abstract}


\section{Introduction}

Three-dimensional images that exceed typical memory size are
increasingly found in a variety of disciplines. BigBrain, for
instance, is a 3D histological image of the human brain that
represents 1~TB of data organized in 3600 planes at full resolution
(1$\mu$m in-plane), and 76~GB at a 40$\mu$m isotropic resolution
commonly used in neurosciences~\cite{amunts2013bigbrain}. Other
examples found in medical imaging, our primary domain of interest,
include \todo{electromicroscopy,spectroscopy-francoise-peyrin}. As
such images would typically be processed on a computing cluster,
possibly using locality-aware file systems such as HDFS, software
libraries are needed to split and merged such images efficiently. We
introduce and compare a family of algorithms to address this problem
in a sequential environment made of a single storage disk. We are
aware of the need for parallel algorithms that would split and merge
images to and from an array of disks but we plan to describe such
algorithms in a subsequent paper based on the results of the present
one.


%also in
%MB/s).

Images are split into chunks representing 3D blocks or 2D slices. We
assume that a block or slice fits in memory. The BigBrain would
perhaps be split into 125 chunks of 600~MB. The decision to split an
image into slices or blocks, and the size of the resulting chunks, may
be constrained by the application. Some applications, for instance
spatial filtering, would commonly require blocks while other ones such
as acquisition artifact removal would rather work on
slices. Flexibility is thus required in the splitting
scheme. Applications processing voxels individually, for instance
histogram computation or k-means clustering, could work on either
slices or blocks.

The main problem encountered in the context of this paper is the fact
that, if not ordered properly, data reads and writes might result in
extensive seek times that, as we illustrate below, drastically limit
the I/O performance. This problem is obviously related to the ordering
of data bytes in image files, which we assume is arbitrary but known
to the algorithm. The problem is also related to the geometry of the
chunks to extract or merge, which we again assume arbitrary but known
to the algorithm. In a nutshell, we are looking for algorithms to
reduce seek times while allowing for arbitrary chunk geometries and
arbitrary byte orders in data files. The main idea of the variations
presented in the remainder of the paper is to convert data orderings
in memory before writing them on disk. 

The literature looks remarkably scarce on the problem described
above. Parallel processing of 3D images has obviously been extensively
studied, but methods have focused more on geometrical approaches to
partition images and on load-balancing and task scheduling techniques
while we aim at algorithms to efficiently split or merge images
regardless of the geometry of the chunks \todo{do a serious literature
review on this topic}. Perhaps this lack of related works could be
attributed to the fact that disk I/O weren't a bottleneck until image
resolution reached a certain threshold. A relevant reference is the
description of the Open Connectome Data Cluster~\cite{burns2013open},
a data warehouse system that allows users to retrieve specific chunks
of large datasets. In this work, the problem of long seek times is
addressed by designing a specific file format based on space-filling
curves, that elegantly preserves spatial proximity on disk. However,
addressing the problem using a specific data organization optimizes
the system for specific chunk geometries, volumetric in this case,
while we aim at algorithms that support multiple types of geometries.

This paper makes the following contributions:
\begin{itemize}
  \item We propose a set of algorithms to split and merge 3D images
    that do not fit in memory.
  \item We model the performance (execution time) of these algorithms,
    based on a simple but realistic disk I/O model.
  \item We evaluate our model and algorithms on a representative
    use-case involving a 1~TB image a hard drive and a solid-state
    drive.
  \item We compare the algorithms in a variety of conditions based on the validated model.
\end{itemize}
Section~\ref{sec:algos} presents our algorithms and their complexity,
Section~\ref{sec:results} details the results and
Sections~\ref{sec:discussion} discusses them.

%\href{http://dl.acm.org/citation.cfm?id=218395}

\newpage

\section{Algorithms}
\label{sec:algos}

\todo{Overlapping blocks. non-cubic blocks.}

In our context, split and merge relate to the same dual
problem \todo{not so sure about this actually}. Without loss of
generality, we focus here on merging for the sake of concision. Our
goal then is to merge a set of $n$ chunks into a single reconstructed
3D image with $R$ voxels of size $b$. We assume that all chunks are of
identical size and that slices are squares and blocks are cubes.

Without loss of generality, we consider a file format where voxels are
written after alphanumerical sorting of their (x,y,z)
representation. That is, x slices are written one after the other, and
in every x slice, y rows are written one after the other. NifTI, the
file format defined by the Neuroimaging Informatics Technology
Initiative Nifti\footnote{\url{https://nifti.nimh.nih.gov}} is such a
format, used by the vast majority of the neuroimaging community. In
the remainder, we use the term ``slice" for ``x slice".

\subsection{Notations}

We adopt the following notations:
\begin{itemize}
\item $R=D^3$: number of voxels in the reconstructed image.
\item $b$: number of bytes per voxel in the reconstructed image and chunks (in B).
\item $n$: number of chunks (blocks or slices).
\item $\alpha_r$: data read rate from disk (in B/s).
\item $\alpha_w$: data write rate to disk (in B/s).
\item $\beta$: disk access time (in s).
\item $\sigma: \mathbb{R}^{+} \rightarrow \mathbb{R}^{+}$: seek time (in s), function of the seek
  distance (in B).
\item $m$: amount of available RAM (in B).
\end{itemize}

The following properties are straightforward applications of these
definitions:
\begin{itemize}
\item number of bytes per chunk: $\frac{Rb}{n}$
\item number of chunks that can fit in memory: $\frac{m.n}{Rb}$
\end{itemize}
For the sake of mathematical simplicity, we assume that
the reconstructed image and blocks are cubic. Under this assumption,
we have the following definitions and properties:
\begin{itemize}
\item Number of slices in a block: $\sqrt[3]{\frac{R}{n}}=d$. This is
  also the number of rows and columns in a slice and the number of
  voxel in a row or column.
\item Number of blocks required to fill a row of the reconstructed
  image: $\sqrt[3]{n}=\nu$.
\end{itemize}
\todo{Make a figure to illustrate the notations.}

\subsection{Disk model}

We use a simplistic disk model where a disk is characterized by its
read and write rates ($\alpha_r$ and $\alpha_w$, in B/s) and its
access time $\beta$ (in s). In particular, we assume that seeks
require a constant amount of time, regardless of the position seeked
to. In practice, large variations would be expected, but modeling such
variations would inevitably lead to models specific to the hardware,
file system or operating system, which we intentionally avoid
here. Thus, our goal is to find algorithms that minimize the
\emph{number} of seek operations. Likewise, in modern systems, read
and write times are greatly impacted by caching occurring at several
levels, which we do not model here.

%% and its seek time,
%% a function $\sigma$ of the seek distance (in B). More precisely,
%% $\sigma$ would typically be in the order of $\beta$ when an opened
%% file is seeked to a random position, but it would be much smaller when
%% a file is seeked to a nearby position.
%% The disk model encompasses file systems, kernel modules and disk
%% controllers required to access the disk from a program run in user
%% space. In modern systems, this often includes caching at various
%% levels \todo{\cite{x}}, which we will model by zeroing $\alpha_r$ or
%% $\alpha_w$ values depending on the application.

\subsection{Slices vs blocks}

Algorithms~\ref{algo:naive-slices} and~\ref{algo:naive-blocks} are the
naive merging methods for slices and blocks.
\begin{algorithm}[h]
\caption{Naive merging from slices.}
\label{algo:naive-slices} 
\begin{algorithmic}
  \FOR{each slice}
    \STATE read slice
    \STATE write slice in reconstructed image
  \ENDFOR      
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[h]
\caption{Naive merging from blocks.}
\label{algo:naive-blocks}
\begin{algorithmic}
  \FOR{each block}
    \STATE read block
    \STATE write block in reconstructed image
  \ENDFOR 
\end{algorithmic}
\end{algorithm}

Algorithms~\ref{algo:naive-slices} and~\ref{algo:naive-blocks}
actually have very different performance even though blocks and slices
have identical sizes. Since slices are stored contiguously in the
reconstructed image, the number of seeks in
Algorithm~\ref{algo:naive-slices} is only $2n$:
\begin{equation}
  N_\mathrm{slices} = 2n 
\end{equation}
However,
Algorithm~\ref{algo:naive-blocks} has to do extra seeks for each row
in each slice of each block. That is, the total number of seeks
performed by Algorithm~\ref{algo:naive-blocks} is:
\begin{equation}
N_\mathrm{blocks} = n+nd^2 
\end{equation}
In
practice, this difference could lead to a tremendous slowdown as we will show
later.

%%  For slices, the total merge time is modeled as follows:
%% \begin{equation}
%% T_\mathrm{slices} = \frac{bR}{\alpha_r}+\frac{bR}{\alpha_w}+2\beta n
%% \end{equation}
%% and for blocks:
%% \begin{equation}
%%   \begin{multlined}
%%     T_\mathrm{blocks} = \frac{bR}{\alpha_r}+\frac{bR}{\alpha_w}+ 2\beta n + \\
%%     n\left({\sigma(bR/2)} + d(d-1)\sigma\left((D-d)b\right)+d\sigma\left((D^2-d^2)b\right) \right)
%%     \end{multlined}
%% \end{equation}

\subsection{Sorted blocks}

Sorting blocks alphanumerically by (x,y,z) as shown on Algorithm 2
reduces the seek time between blocks. 

\begin{algorithm}[h]
  \caption{Merging from sorted blocks.}
  \label{algo:sorted-blocks}
  \begin{algorithmic}
    \STATE list = sort blocks according to (x,y,z)
    \FOR{each block in list}
      \STATE read block
      \STATE write block in reconstructed image
    \ENDFOR
  \end{algorithmic}
  \end{algorithm}

\todo{Not sure what we are measuring here. The number of seeks will be identical. The difference in seek time is supposed to be very minimal.}

\subsection{Buffered slices}

The algorithms presented up to now are in fact particular cases of
memory buffering where the amount of available memory equals the
maximum size of a chunk. 

\begin{algorithm}[h]
  \caption{Buffered merging from slices}
  \label{algo:buffered-slices}
  \begin{algorithmic}
    \STATE list = sort slices by increasing x values
    \STATE initialize buffer
    \STATE index = 0
    \WHILE{index\textless n}
      \WHILE{sizeof(buffer)\textless m}
        \STATE slice = list[index]
        \STATE read slice and append it to buffer
        \STATE index += 1
       \ENDWHILE
       \STATE write buffer in reconstructed image
       \STATE clear buffer
    \ENDWHILE
  \end{algorithmic}
\end{algorithm}

\begin{equation}
N_\mathrm{buff\_slices} =  n + \ceil*{\frac{bR}{m}}
\end{equation}


\subsection{Buffered blocks: Cluster reads}


As detailed in Algorithm~\ref{algo:cluster-reads}, cluster reads load
multiple blocks in memory, concatenate them in a buffer and write the
buffer sequentially in the reconstructed image. The buffer is a
slightly complex data structure, for instance an associative array or
a Python dictionary, capable of storing multiple disjoint sequences of
contiguous bytes without having to allocate memory for the bytes
between such sequences. Figure~\ref{fig:cluster-reads-buffer}
illustrates how the buffer would fill up when two blocks of one slice
and three rows per slice are loaded. Implementation details about the
buffer are given later.
\begin{figure}
\centering
\def\svgwidth{\columnwidth}
\input{figures/buffer-cluster-reads.pdf_tex}
\caption{Illustration of the buffer used in cluster reads. White
  portions are not allocated. [a,b] refers to block a, row b.}
\label{fig:cluster-reads-buffer}
\end{figure}
\begin{algorithm}[h]
  \caption{Buffered merging of blocks with cluster reads}
  \label{algo:cluster-reads}
  \begin{algorithmic}
    \STATE list = sort blocks by increasing (x,y,z) values
    \STATE initialize buffer
    \STATE index = 0
    \WHILE{index\textless n}
      \WHILE{sizeof(buffer)\textless m}
        \STATE block = list[index]
        \STATE read block and insert it in buffer
        \STATE index += 1
        \ENDWHILE
       \STATE write buffer in reconstructed image
       \STATE clear buffer
    \ENDWHILE
  \end{algorithmic}
\end{algorithm}

The number of seeks performed by cluster reads depends on how blocks
loaded in memory arrange in the reconstructed image. In the best case,
complete contiguous slices of the reconstructed image can be assembled
in memory and written without any seeking. In the worst case, some
rows in the reconstructed image are not covered by blocks loaded in
memory: O($d^2$) seeks are then required to write the memory load, one
for every partial row in every partial slice. In the intermediary
case, rows are complete but some slices can only be partially
reconstructed: O($d$) seeks are then
required. We have:
\begin{equation}
  N_\mathrm{cluster\_reads}= n + \sum_{i=1}^{\ceil*{\frac{bR}{m}}}f_i
\end{equation}
where $f_i$ is the number of seeks done in the $i^{th}$ memory
load. Figure~\ref{fig:cluster-reads} lists the possible block
arrangements and the corresponding values of $f_i$. \todo{Val\'erie: Check that
  ordering is consistent with Nifti's.}
%% In case (1) on
%% this Figure, the memory load completely fills rows and slices in the
%% reconstructed image. Thus it can be written with a single seek,
%% $f_i$=1. In case (2), all the rows are complete but $d$ slices are
%% incomplete; one seek is required for each slices, $f_i=d$. In case
%% (3), $d$ rows of $d$ slices are incomplete, $f_i=d^2$. In case (4),
%% $2d$ rows of $d$ slices are incomplete, $f_i=2d^2-d$. In case (5),
%% $2d$ slices are incomplete but no row is incomplete, $f_i=2d$. In case
%% (6), $d$ rows of $d$ slices are incomplete, and $d$ additional slices
%% are incomplete, $f_i$=$d^2+d-1$. And in case (7), $d$ rows of $2d$
%% slices are incomplete, $f_i$=$2d^2-1$.
In general:
\begin{equation}
  N_\mathrm{cluster\_reads}= n + \ceil*{\frac{bR}{m}}O(d^2)
\end{equation}

\begin{figure}
\centering
\def\svgwidth{0.3\columnwidth}
\input{figures/case1.pdf_tex}
\def\svgwidth{0.3\columnwidth}
\input{figures/case2.pdf_tex}
\def\svgwidth{0.3\columnwidth}
\input{figures/case3-a.pdf_tex}
\def\svgwidth{0.3\columnwidth}
\input{figures/case3-b.pdf_tex}
\def\svgwidth{0.3\columnwidth}
\input{figures/case4.pdf_tex}
\def\svgwidth{0.3\columnwidth}
\input{figures/case5.pdf_tex}
\def\svgwidth{0.3\columnwidth}
\input{figures/case6-a.pdf_tex}
\def\svgwidth{0.3\columnwidth}
\input{figures/case6-b.pdf_tex}
\def\svgwidth{0.3\columnwidth}
\input{figures/case7.pdf_tex}

\caption{In-memory block arrangements leading to different number of
  seeks in cluster reads. Blue blocks need seeking before each of
  their slices ($d$ seeks in total). Red blocks need seeking before
  each of their rows ($d^2$ seeks). Purple blocks require seeking
  before each row and each slice ($d^2$ seeks). Green blocks are
  written with no seeking.}
\label{fig:cluster-reads}
% See source files in
% https://drive.google.com/drive/folders/0BzT7pbvPDUdQRXppVlhfdS0ycGc?usp=sharing
\end{figure}

If $m\geq\frac{Rb}{\sqrt[3]{n}^2}$, i.e., a
full row of blocks fits in memory, then a
variant adjusts the memory load to the
highest value for which only full rows are written. In this
case, $f_i \in \{1,d,2d\}$ and:
\begin{equation}
  N_\mathrm{cluster\_reads\_v1}= n + \frac{bR}{m|}O(d)
\end{equation}
where
$m|=\frac{Rb}{\sqrt[3]{n}^2}\floor*{\frac{m\sqrt[3]{n}^2}{Rb}}$. It
should be noted that $m|$ is lower than $m$, therefore
$N_\mathrm{cluster\_reads\_v1}$ might be greater than
$N_\mathrm{cluster\_reads}$ for small values of $d$.

If $m\geq\frac{Rb}{\sqrt[3]{n}}$, i.e., a full slice of blocks
fits in memory, then a second variant adjusts
the memory load to the highest value for which only full
slices are written. In this case:
\begin{equation}
  N_\mathrm{cluster\_reads\_v2}= n + \frac{bR}{m||}
\end{equation}
where $m||=\frac{Rb}{\sqrt[3]{n}}\floor*{\frac{m\sqrt[3]{n}}{Rb}}$.
    
\subsection{Buffered blocks: Multiple reads}

In multiple reads, blocks are read partially to ensure that the memory
buffer only contains contiguous bytes, at any time. That is, a given
block will be read multiple times.

\begin{algorithm}[h]
  \caption{Buffered merging from blocks using multiple reads
    \todo{improve notations and algo}.}
  \label{algo:multiple-reads}
  \begin{algorithmic}
    \STATE list = sort blocks by increasing (x,y,z) values
    \STATE initialize buffer
    \STATE pointer = 0, max\_index=(m-1)
    \WHILE{max\_index \textless sizeof(reconstructed\_image)}
    \WHILE{pointer \textless max\_index}
        \STATE block = read block that contains voxel index\_to\_voxel(pointer)
        \STATE insert in buffer all voxels (x,y,z) of block where voxel\_to\_index(x,y,z) \textless max\_index
        \STATE update pointer to point to the next unassigned element in buffer
        \ENDWHILE
       \STATE write buffer in reconstructed image
       \STATE clear buffer
       \STATE pointer = max\_index + 1
       \STATE max\_index += m
    \ENDWHILE
  \end{algorithmic}
\end{algorithm}

\begin{equation}
  N_\mathrm{multiple\_reads}= n\left( \ceil*{\frac{\left((d-1)D^2+dD\right)b}{m}}\right) + 2\ceil*{\frac{bR}{m}}
\end{equation}

\subsection{Lossless compression}
     
Lossless compression might reduce overall read and write times by
reducing seek time and data sizes. However, it needs to be implemented
on-the-fly, i.e., in chunks. See the work in~\cite{rajna2015speeding}
and
\href{https://github.com/pauldmccarthy/indexed\_gzip}{corresponding
  implementation}. This is straightforward to apply for random reads
but not for random writes. Multiple reads is the only merging strategy
that works with literally no seek in the reconstructed image which
could therefore potentially use in-memory compression.

\todo{could we merge without de-compressing?}

\section{Benchmark}
\label{sec:benchmark}

\subsubsection{Implementation}

Our algorithms are implemented in Python using the Nibabel
library. \todo{Val\'erie and Yongping: (1) extract ImageUtils from
  mapreduce repository (preserve git history; just copy the repo and
  remove the mapreduce stuff in the copy). Link to the github
  repo. (2) Describe the code briefly, in particular the
  implementation tricks used for each implementation.}

\subsubsection{Data}
To benchmark our library we use the 76~GB BigBrain image split in 125
non-overlapping chunks. The data is available \todo{[here]}.

\subsubsection{Infrastructure}

We run experiments using a Dell Precision Tower 3620 with CentOS Linux release
7.3.1611, 32~GB of RAM and two disks:
\begin{itemize}
  \item Hard disk drive (HDD): HGST Travelstar 7K1000, 7200~rpm, 931GiB (1TB), firmware version JB0OA3W0.
    \item Solid-state drive (SSD): SanDisk X400 2.5, 238GiB (256GB),
      firmware version X4130012.
\end{itemize}
Both drives used 512-byte logical sectors, 4096-byte physical sectors,
SATA >3.1 (6.0 Gb/s) and were accessed through the XFS file system.
%smartctl --xall /dev/sda

\section{Results}
\label{sec:results}

\subsection{Slices vs blocks}

\todo{Valerie's results at PERFORM + model.}

\subsection{Sorted blocks vs blocks}

\subsection{Buffered slices vs slices}


\subsection{Cluster reads vs Multiple reads vs Buffered slices}


\subsection{Lossless Compression}

\subsection{Splitting}

\newpage

\section{Discussion}
\label{sec:discussion}

What is the best algorithm (between 3 and 4), in which context. Did we
solve the seek problem or is there any issue remaining?

File formats:
\begin{itemize}
\item would MINC~\cite{vincent2016minc} support such algorithms? See on-going email discussion
with P. Bellec.
\item some formats optimize the storage for particular split shapes. See
  ndstore~\cite{burns2013open}. Such formats would behave equally
  poorly with chunks that do not comply to this geometry. Our
  algorithms would work equally well with such formats.
\end{itemize}

The performance model might be dependent on the particular
infrastructure (disks, OS and filesystem) used in the experiments. In
particular, caching might behave differently in a different setup.

Reconstructing (therefore splitting) a complete 3D image may not
always be necessary. Re-splitting, for instance, could be done from
existing chunks.

\section{Conclusion}

Future work:
- obliques
- parallel algorithms
- re-splitting
- model

\section*{Acknowledgment}

(If not added as co-authors): Greg Kiar, Pierre Bellec.

\bibliographystyle{IEEEtran}
\bibliography{IEEEabrv,biblio.bib}

\end{document}
