\documentclass[10pt, conference, compsocconf]{IEEEtran}

% packages
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{amsfonts} % for R symbol (the set of real numbers)
\usepackage{color}
\usepackage[pdftex]{graphicx}
\usepackage{hyperref}
\usepackage{mathtools}
\usepackage{multirow}
\usepackage{stmaryrd} % for llbracket and rrbracket
\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}
\DeclarePairedDelimiter{\floor}{\lfloor}{\rfloor}

% new commands
\newcommand{\todo}[1]{\marginpar{\parbox{18mm}{\flushleft\tiny\color{red}\textbf{TODO}:
      #1}}}

\newcommand{\note}[1]{
  \color{blue}\emph{[Note: #1]}
  \color{black}
}


\begin{document}

\title{Sequential algorithms to split and merge ultra-high resolution 3D images}

\author{Final ordering
  to be discussed: Yongping Gao/Val\'erie Hayot-Sasson,
  Yuhong Yan, Tristan Glatard\\
  Department of Computer Science and Software Engineering\\ Concordia University, Montreal, Quebec, Canada\\
  {first.last}@concordia.ca
}

\maketitle

\begin{abstract}
  Splitting and merging data is a requirement for most parallel or
  distributed processing operations. Naive algorithms, however,
  perform very poorly to split or merge 3D blocks from ultra-high
  resolution images, due to important seek times. We propose and
  analyze two algorithms based on in-memory reconstruction of image
  parts. Cluster reads access image chunks only once while Multiple
  reads minimize the accesses to the image. Evaluation on a
  3000x3000x3000 brain image shows that our algorithms perform
  similarly to the optimal configuration (slice merging) provided that
  enough memory is available, which solves our initial problem.  We
  conclude that splitting and merging large 3D images can be done
  efficiently based on simple data formats.
\end{abstract}


\section{Introduction}

% Problem definition
Three-dimensional images that exceed typical memory size are
increasingly found in a variety of disciplines. Big~Brain, for
instance, is a 3D histological image of the human brain that
represents 1~TB of data organized in 3600 planes at full resolution
(1$\mu$m in-plane), and 76~GB at a 40$\mu$m isotropic resolution
commonly used in neurosciences~\cite{amunts2013bigbrain}. This dataset
is being re-scanned at an even higher resolution, leading to 100~TB of
raw data. Other examples found in medical imaging, our primary domain
of interest, include high-resolution 3D electron microscopy (see,
e.g., ~\cite{bock2011network}) or micro- and
nano-tomography~\cite{10.1371/journal.pone.0035691}. As such images
would typically be processed on a computing cluster, possibly using
locality-aware file systems such as HDFS~\cite{shvachko2010hadoop},
efficient software libraries are needed to split and merge them
efficiently. In particular, data reads and writes might result in
extensive seek times that drastically limit I/O performance.  In this
paper we introduce and compare a family of sequential algorithms to
split and merge images with minimal seek time.

Images are split into chunks representing 3D blocks or 2D slices,
which we assume fit in memory. A dataset such as Big~Brain would
perhaps be split into 125 chunks of 600~MB. The decision to split an
image into slices or blocks, and the size of the resulting chunks, is
usually done by the application. Some applications would commonly
require blocks, for instance spatial filtering, while other ones such
as acquisition artifact removal would rather work on
slices. Applications processing voxels individually, for instance
histogram computation or k-means clustering, could work on either
slices or blocks. Flexibility is thus required in the splitting
scheme.

Seek time is highly dependent on the byte ordering in image files,
which we assume is arbitrary but known to the algorithm. The geometry
of the chunks to split or merge (slices or blocks) and their sizes are
also critical, which we again assume arbitrary but known to the
algorithm.  The main idea of our algorithms will be to convert data
orderings in memory before writing them to disk.

The literature on this problem is remarkably scarce. Parallel and
distributed image processing has obviously been extensively studied
and used in various
platforms~\cite{miguet1991elastic,tang2007eman2,yang2008parallel,braunl2013parallel,moise2013terabyte,bajcsy2013terabyte},
but methods have focused more on geometrical approaches to partition
images and on load-balancing and task scheduling techniques while we
aim at algorithms to efficiently split or merge images regardless of
the geometry of the chunks. Seek times are, however, often identified
as an issue but the preferred solution is usually to optimize data
storage formats for a certain application while we are searching for
algorithms that would reduce the seek time regardless of the data
format. For instance, the Open Connectome Data
Cluster~\cite{burns2013open} is a data warehouse system that allows
users to retrieve specific chunks of large image datasets. It
addresses seek times by designing a specific file format based on
space-filling curves, which elegantly preserves spatial proximity on
disk. Such specific data formats optimize the system for particular
chunk geometries, blocks in this case, while we aim at algorithms to
support multiple types of geometries.
%http://dl.acm.org/citation.cfm?id=218395 also minimizes seek time using a specific format.

This paper makes the following contributions:
\begin{itemize}
  \item We propose a set of algorithms to split and merge
    terabyte-scale 3D images from blocks or slices.
  \item We model the performance (number of seeks) of these algorithms.
  \item We evaluate our model and algorithms on a representative
    use-case involving a 1~TB image a hard drive and a solid-state
    drive.
\end{itemize}
Section~\ref{sec:algos} presents our algorithms and their complexity,
Section~\ref{sec:results} details the results and
Sections~\ref{sec:discussion} discusses them.\todo{Revise when outline is final.}

%\href{http://dl.acm.org/citation.cfm?id=218395}

\section{Algorithms}
\label{sec:algos}

Split and merge relate to the same dual problem in our context. We
focus here on merging for the sake of concision. Splitting algorithms
can be derived from merging ones by swapping reads and writes. Our
goal then is to merge a set of $n$ chunks into a single reconstructed
3D image with $R$ voxels of size $b$. For simplicity, we assume that
all chunks are of identical size, that they do not overlap, and that
slices are squares and blocks are cubes.

We consider a file format where voxels are written in column-major
order, that is, for an image of dimensions (i, j, k), i is the fastest
changing dimension, followed by dimension j, followed by dimension
k. All voxels in a \emph{slice} have a the same $k$ and all voxels in
a \emph{row} have the same $j$.  NIfTI, the file format defined by the
Neuroimaging Informatics Technology Initiative
NIfTI\footnote{\url{https://nifti.nimh.nih.gov}} is such a format,
used by the vast majority of the neuroimaging community.

\subsection{Notations}

We adopt the following notations (see Figure\ref{fig:notations}):
\begin{itemize}
\item $R=D^3$: number of voxels in the reconstructed image.
\item $b$: number of bytes per voxel (in B).
\item $n$: number of chunks (blocks or slices).
\item $m$: amount of available memory (in B).
\item $m'$: amount of used memory (in B), $m'\leq m$.
\end{itemize}
We also have the following relations:
\begin{itemize}
\item Number of slices, rows, columns in a block: $\sqrt[3]{\frac{R}{n}}=d$.
\item Number of blocks in a block row: $\sqrt[3]{n}$.
\end{itemize}

\subsection{Disk model}

A disk is characterized by its read and write rates and its seek
time. We do not distinguish between access time and seek time. We also
assume that seeks require a constant amount of time, regardless of the
position seeked to. In practice, large variations would be expected,
but modeling such variations would inevitably lead to models specific
to the hardware, file system or operating system, which we
intentionally avoid here. Likewise, in modern systems, read and write
times are greatly impacted by caching occurring at several levels,
which we do not model here. Thus, our goal is to find algorithms that
minimize the \emph{number} of seek operations.

\todo{Add ballpark values of seek time (5ms) and read/write rates.}

\subsection{Slices vs blocks}

\begin{figure}
  \centering
  \begin{minipage}[b]{0.42\columnwidth}
    \def\svgwidth{0.8\columnwidth}
    \input{figures/svg/Notations.pdf_tex}
    \caption{Notations. A \emph{block row} is shown in red. A
      \emph{block slice} is shown in blue.}
    \label{fig:notations}
  \end{minipage}
  \quad \quad \quad
  \begin{minipage}[b]{0.42\columnwidth}
    \def\svgwidth{0.8\columnwidth}
    \input{figures/svg/buffer.pdf_tex}
    \caption{Buffer used in cluster reads (d=4).  White portions in
      the buffer are not allocated. }
    \label{fig:cluster-reads-buffer}
  \end{minipage}
\end{figure}
Algorithms~\ref{algo:naive-slices} and~\ref{algo:naive-blocks} show
the naive merging methods for slices and blocks. These algorithms
actually have very different complexities even though blocks and
slices have identical sizes. Since slices are stored contiguously in
the reconstructed image, the number of seeks in
Algorithm~\ref{algo:naive-slices} is only $2n$ as $n$ seeks are
required to read the slices and $n$ seeks are required to write them:
\begin{equation}
  N_\mathrm{slices} = 2n 
\end{equation}
However,
Algorithm~\ref{algo:naive-blocks} has to do extra seeks for each row
in each slice of each block:
\begin{equation*}
N_\mathrm{blocks} = n+nd^2 
\end{equation*}
or, using $R$ and $n$ as main variables:
\begin{equation}
N_\mathrm{blocks} = n+n\left(\sqrt[3]{\frac{R}{n}}\right)^2
\end{equation}
In practice, this difference could lead to a tremendous slowdown, as
we will show later.

\begin{algorithm}
\caption{Naive merging from slices}
\label{algo:naive-slices} 
\begin{algorithmic}
  \FOR{each slice}
    \STATE read slice
    \STATE write slice in reconstructed image
  \ENDFOR      
\end{algorithmic}
\end{algorithm}
\begin{algorithm}[h]
\caption{Naive merging from blocks}
\label{algo:naive-blocks}
\begin{algorithmic}
  \FOR{each block}
    \STATE read block
    \STATE write block in reconstructed image
  \ENDFOR 
\end{algorithmic}
\end{algorithm}
%%  For slices, the total merge time is modeled as follows:
%% \begin{equation}
%% T_\mathrm{slices} = \frac{bR}{\alpha_r}+\frac{bR}{\alpha_w}+2\beta n
%% \end{equation}
%% and for blocks:
%% \begin{equation}
%%   \begin{multlined}
%%     T_\mathrm{blocks} = \frac{bR}{\alpha_r}+\frac{bR}{\alpha_w}+ 2\beta n + \\
%%     n\left({\sigma(bR/2)} + d(d-1)\sigma\left((D-d)b\right)+d\sigma\left((D^2-d^2)b\right) \right)
%%     \end{multlined}
%% \end{equation}


\subsection{Buffered slices}

Algorithm~\ref{algo:naive-slices} is a particular case of memory
buffering where the amount of available memory equals the maximum size
of a chunk. More buffering can be achieved when the amount of
available memory increases, as shown in
Algorithm~\ref{algo:buffered-slices}.
\begin{algorithm}[h]
  \caption{Buffered merging from slices}
  \label{algo:buffered-slices}
  \begin{algorithmic}[1]
    \STATE sorted\_slices = sort slices by increasing k values
    \STATE initialize buffer
    \FOR{i = 0 ; i \textless n ; i+=1 }
      \STATE slice = sorted\_slices[i]
      \IF{\texttt{sizeof}(buffer)+\texttt{sizeof}(slice) $\geq$ m}
        \STATE write buffer in reconstructed image
        \STATE clear buffer
      \ENDIF
      \STATE read slice and append it to buffer
    \ENDFOR
  \end{algorithmic}
\end{algorithm}
This algorithm writes in the reconstructed image using a single seek
per memory load. Therefore:
\begin{equation}
N_\mathrm{buff\_slices} =  n + \ceil*{\frac{bR}{m}}
\end{equation}
Buffered slices are straightforward to implement, however, their
extension to block merging is not easy. The remainder of this Section
presents our attempts for such a generalization.

\subsection{Buffered blocks: Cluster reads}

Cluster reads are the more direct extension of buffered slices to
blocks: they load multiple blocks in memory, concatenate them in a
buffer and write the buffer in the reconstructed image. Seeking is
reduced compared to naive block merging since contiguous parts of the
buffer will be written without seeking. A given
block is accessed only once.

The buffer might be a slightly complex data structure, for instance an
associative array or a Python dictionary, capable of storing multiple
disjoint sequences of contiguous bytes without having to allocate
memory for the bytes between such
sequences. Figure~\ref{fig:cluster-reads-buffer} illustrates how the
buffer would fill up for the two first blocks in a reconstructed
image, assuming that blocks are of size 4x4x4.

The number of seeks performed by cluster reads depends on how blocks
loaded in memory arrange in the reconstructed image. In the best case,
complete contiguous slices of the reconstructed image can be assembled
in memory and written in a single seek. In the worst case, the
memory load only partially covers rows in the reconstructed image:
O($d^2$) seeks are then required during writing, one for every partial
row in every partial slice. In the intermediary case, rows are
complete but some slices can only be partially reconstructed: O($d$)
seeks are then required.

\begin{figure}
\centering
\def\svgwidth{0.3\columnwidth}
\input{figures/svg/case1-a.pdf_tex}
\def\svgwidth{0.3\columnwidth}
\input{figures/svg/case2-a.pdf_tex}
\def\svgwidth{0.3\columnwidth}
\input{figures/svg/case3.pdf_tex}
\caption{Memory-load configurations in cluster reads, leading to
  different number of seeks. Red blocks need seeking before each of
  their rows ($d^2$ seeks). Blue blocks need seeking before each of
  their slices ($d$ seeks in total). Green blocks need only a single
  seek. Grey, dashed, transparent blocks represent the contiguous
  memory loads and are added for the sake of visualization.}
\label{fig:cluster-reads}
\end{figure}
Our cluster reads algorithm focuses on the three memory load
configurations represented in Figure~\ref{fig:cluster-reads}, that is,
the amount of memory $m'$ used by the algorithm is rounded down to the
closest number of complete blocks (case 1), of complete block rows (case
2) or of complete block slices (case 3). This is in general reasonable
since adding an incomplete row to a set of complete ones multiplies
the number of required seeks by $d$, as illustrated in
Figure~\ref{fig:avoided-configurations}-Left. In some cases though,
rounding $m$ down to $m'$ might increase the number of required
memory loads to a point that the overall number of seeks also
increases. Such cases are, however, slightly unusual and their
complete description requires extensive calculations involving modulo
arithmetic, which we felt were unwieldy to report here.
\begin{figure}
  \centering
\def\svgwidth{0.3\columnwidth}
\input{figures/svg/incomplete-rows.pdf_tex}
\quad \quad \quad
\def\svgwidth{0.3\columnwidth}
\input{figures/svg/overlap.pdf_tex}
\caption{Configurations that increase the number of seeks per
  memory-load and are thus deliberately avoided by cluster
  reads. Left: configuration with incomplete block rows (multiplies
  number of seeks by $d$). Right: configuration with block rows that
  overlap multiple block slices (multiplies number of seeks by $2$).}
\label{fig:avoided-configurations}
\end{figure}

Our algorithm also avoids configurations where the memory load
overlaps multiple block slices in case 2 or multiple block rows in
case 1, as such overlaps multiply the number of required seeks (see
Figure~\ref{fig:avoided-configurations}-Right).

Cluster reads are described in
Algorithm~\ref{algo:cluster-reads}. Function \texttt{switch} (line 3)
selects one of the three cases based on the amount of available memory
and the number of blocks. It returns \texttt{m'} and \texttt{case},
the identifier of the selected case. Function \texttt{check\_overlap}
(line 7) determines whether two blocks overlap multiple block slices
(case 2) or multiple block rows (case 1). For case 3 it always returns
false.  Function \texttt{sizeof} (line 8) returns the actual memory
used by its argument, including only its allocated segments in case
the argument is a buffer.
\begin{algorithm}[h]
  \caption{Buffered merging of blocks with cluster reads}
  \label{algo:cluster-reads}
  \begin{algorithmic}[1]
    \STATE sorted\_blocks = sort blocks by increasing (k,j,i)
    \STATE initialize buffer
    \STATE (m',case)=switch(m,n,R,b)
    \STATE old\_block = sorted\_blocks[0]
    \FOR{i = 0 ; i\textless n ; i+=1}
      \STATE block = sorted\_blocks[i]
      \STATE overlap = check\_overlap(block,old\_block,case)
      \IF{sizeof(buffer)+sizeof(block) $\geq$ m' \textbf{or} overlap=true}
      \STATE write buffer in reconstructed image
      \STATE clear buffer
      \STATE overlap = false
      \ENDIF
      \STATE read block and insert it in buffer
      \ENDFOR
  \end{algorithmic}
\end{algorithm}

The amount of memory used $m'$ is set as follows in each of the 3 cases:
\begin{equation*}
  m_1' = \frac{Rb}{n}\floor*{\frac{mn}{Rb}};
  m_2' = \frac{Rb}{\sqrt[3]{n}^2}\floor*{\frac{m\sqrt[3]{n}^2}{Rb}};
  m_3' = \frac{Rb}{\sqrt[3]{n}}\floor*{\frac{m\sqrt[3]{n}}{Rb}}
\end{equation*}
The number of seeks performed by cluster reads in each of the three cases is:
\begin{equation*}
  N^i_{\mathrm{CR}} = n + x_ib_i, \quad i \in \llbracket 1, 3\rrbracket,
\end{equation*}
where $x_i$ is the number of memory loads required to reconstruct the
image and $b_i$ is the number of seeks required to write the memory
load. The first $n$ seeks in the equation are required to read all the
blocks once. According to Figure~\ref{fig:cluster-reads}, we have:
\begin{equation*}
  b_1=d^2=\sqrt[3]{\frac{R}{n}}^2 \quad ; \quad b_2=d=\sqrt[3]{\frac{R}{n}} \quad ; \quad b_3=1
\end{equation*}
The
numbers of memory loads required to reconstruct the image are:
\begin{equation*}
  x_1 = \ceil*{\frac{Rb}{\sqrt[3]{n}^2m_1'}}\sqrt[3]{n}^2;
  x_2 = \ceil*{\frac{Rb}{\sqrt[3]{n}m_2'}}\sqrt[3]{n};
  x_3 = \ceil*{\frac{Rb}{m_3'}}
\end{equation*}
Because our algorithm avoids overlapping configurations, $x_1$ is
proportional to the total number of block rows in the image
($\sqrt[3]{n}^2$) and $x_2$ is proportional to the total number of
block slices ($\sqrt[3]{n}$).

Finally, the total number of seeks
performed by cluster reads to reconstruct the image is:
\begin{equation}
N_\mathrm{CR} =
\begin{cases}
  n + \ceil*{\frac{Rb}{\sqrt[3]{n}^2m_1'}}\sqrt[3]{R}^2     & \text{if } m < \frac{Rb}{\sqrt[3]{n}^2}\\[0.4em]
  n + \ceil*{\frac{Rb}{\sqrt[3]{n}m_2'}}\sqrt[3]{R}        & \text{if } \frac{Rb}{\sqrt[3]{n}^2} \leq m < \frac{Rb}{\sqrt[3]{n}}\\[0.4em]
  n + \ceil*{\frac{Rb}{m_3'}}                              & \text{if } \frac{Rb}{\sqrt[3]{n}} \leq Rb < n\\[0.4em]
  n + 1                                                          & \text{otherwise}
\end{cases} \label{eq:seeks-cluster-reads-1}
\end{equation}
It should be noted that $N_{CR}$ is not a continuous function of m,
due to the differences among $b_i$ values.

%% In case (1) on
%% this Figure, the memory load completely fills rows and slices in the
%% reconstructed image. Thus it can be written with a single seek,
%% $f_i$=1. In case (2), all the rows are complete but $d$ slices are
%% incomplete; one seek is required for each slices, $f_i=d$. In case
%% (3), $d$ rows of $d$ slices are incomplete, $f_i=d^2$. In case (4),
%% $2d$ rows of $d$ slices are incomplete, $f_i=2d^2-d$. In case (5),
%% $2d$ slices are incomplete but no row is incomplete, $f_i=2d$. In case
%% (6), $d$ rows of $d$ slices are incomplete, and $d$ additional slices
%% are incomplete, $f_i$=$d^2+d-1$. And in case (7), $d$ rows of $2d$
%% slices are incomplete, $f_i$=$2d^2-1$.

   
\subsection{Buffered blocks: Multiple reads}

Multiple reads are shown in Algorithm~\ref{algo:multiple-reads}.  The
main idea of this algorithm is that blocks are read partially (line 9)
to ensure that the memory buffer only contains contiguous
bytes. Therefore, the buffer can be written continuously to the
reconstructed image, without seeking (line 13).  However, a given
block might be read multiple times, in different memory loads.

\begin{figure}
\centering
\def\svgwidth{0.3\columnwidth}
\input{figures/svg/mreads-case1.pdf_tex}
\def\svgwidth{0.3\columnwidth}
\input{figures/svg/mreads-case2.pdf_tex}
\def\svgwidth{0.3\columnwidth}
\input{figures/svg/mreads-case3.pdf_tex}

\medskip

\def\svgwidth{0.3\columnwidth}
\input{figures/svg/mreads-case4.pdf_tex}
\def\svgwidth{0.3\columnwidth}
\input{figures/svg/mreads-case5.pdf_tex}
\caption{Memory-load configurations in multiple reads (d=4, D=16,
  n=64, $k_1=k_2=k_3=k_4=k_5=1$). Red colour shows the content of the
  memory load. Small dots depict block frontiers and long dashes
  depict rows and slices within blocks.}
\label{fig:multiple-reads-cases}
\end{figure}



\begin{algorithm}[h]
  \caption{Buffered merging of blocks with multiple reads}
  \label{algo:multiple-reads}
  \begin{algorithmic}[1]
  \STATE sorted\_blocks = sort blocks by increasing (k,j,i)
  \STATE start\_index = 0 ; end\_index=(m-1)
  \STATE write\_range = (start\_index, end\_index)
  \WHILE{end\_index \textless Rb}
    \STATE initialize buffer
    \FOR{split in sorted\_blocks}
      \IF{split has voxels in write\_range}
        \STATE split\_data = read split
        \STATE in split\_data, extract the rows in write\_range
        \STATE insert rows in buffer
      \ENDIF
    \ENDFOR
    \STATE write buffer to reconstructed\_image
    \STATE start\_index = end\_index + 1 ; end\_index += m
  \ENDWHILE

  \end{algorithmic}
\end{algorithm}

As illustrated in Figure~\ref{fig:multiple-reads-cases}, we assume
that $m'$ represents an integer number $k$ of sub-rows (case 1,
$k_1<\sqrt[3]{n}$), of complete rows (case 2, $k_2<d$), of tile rows
(case 3, $k_3<\sqrt[3]{n}$), of slices (case 4, $k_4<d$) or of block
slices (case 5, $k_5<\sqrt[3]{n}$). In each of these 5 cases, we
define $v_i$ as follows:
\begin{equation*}
  v_1=db \text{ ; }  v_2=Db \text{ ; } v_3=Ddb \text{ ; } v_4=D^2b \text{ ; } v_5=D^2db
\end{equation*}
so that we have:
\begin{equation*}
k_i=\floor*{\frac{m}{v_i}} \quad \mathrm{and} \quad m_i'=k_iv_i, \quad i \in \llbracket 1,5 \rrbracket
\end{equation*}

The total number of seeks performed by multiple reads in case $i$ is:
\begin{eqnarray*}
  N^i_{\mathrm{MR}} &=& x_i + (x_i-1)b_i +b_i', \quad i \in \llbracket 1,5 \rrbracket\\
  &=& x_i \left(1+b_i\right)-b_i+b_i'
\end{eqnarray*}
where $x_i$ is the total number of memory loads, $b_i$ is the
number of blocks accessed by the first ($x_i$-1) memory loads and
$b_i'$ is the number of blocks access by the last memory load. The
first $x_i$ seeks in the equation are required to write each memory
load (1 seek per memory load). We have:
\begin{equation*}
  x_i = \ceil*{\frac{Rb}{m_i'}}, \quad i \in \llbracket 1,5 \rrbracket
\end{equation*}
and:
\begin{equation*}
b_1=k_1 ; \quad b_2=\sqrt[3]{n} ; \quad b_3 = k_3\sqrt[3]{n} ; \quad b_4=\sqrt[3]{n}^2 ; \quad b_5=k_5\sqrt[3]{n}^2
\end{equation*}
and:
\begin{eqnarray*}
b_1'=\sqrt[3]{n}D^2\text{ mod }k_1; &b_2'=b_2;   & b_3' = \sqrt[3]{n}\left( \sqrt[3]{n}D\text{ mod }k_3\right)\\
                                   &b_4'=b_4;  & b_5'=\sqrt[3]{n}^2\left( \sqrt[3]{n}\text{ mod }k_5\right) 
\end{eqnarray*}
It gives the following expression for $N_\mathrm{MR}$:
\begin{equation*}
  N_\mathrm{MR} =
\begin{cases}
  \ceil*{\frac{Rb}{m_1'}}(k_1+1)-k_1 \\
  + \left( \sqrt[3]{n}D^2 \text{ mod } k_1 \right)& \text{\hspace*{-0.9em}if } d \leq \frac{m}{b} < D\\[0.4em]
  
  \ceil*{\frac{Rb}{m_2'}}(\sqrt[3]{n}+1)   & \text{\hspace*{-0.9em}if } D \leq \frac{m}{b} < Dd\\[0.4em]
  
  \ceil*{\frac{Rb}{m_3'}}(k_3\sqrt[3]{n}+1)-k_3\sqrt[3]{n}+\\[0.4em]
  \quad \quad \quad \quad \sqrt[3]{n}\left(\sqrt[3]{n}D\text{ mod } k_3\right)& \text{\hspace*{-0.9em}if } Dd \leq \frac{m}{b} < D^2\\[0.4em]
  \ceil*{\frac{Rb}{m_4'}}(\sqrt[3]{n}^2+1) & \text{\hspace*{-0.9em}if } D^2 \leq \frac{m}{b} < D^2d\\[0.4em]
  \ceil*{\frac{Rb}{m_5'}}(k_5\sqrt[3]{n}^2+1)-k_5\sqrt[3]{n}^2+&\\[0.4em]
  \quad \quad \sqrt[3]{n}^2\left(\sqrt[3]{n} \text{ mod } k_5\right)&\
  \text{\hspace*{-0.9em}if } D^2d \leq \frac{m}{b} < R
\end{cases}
\end{equation*}
And finally, using $R$ and $n$ as main variables:
\begin{equation}
N_\mathrm{MR} =
\begin{cases}
  \ceil*{\frac{Rb}{m_1'}}\left(\frac{m_1'\sqrt[3]{n}}{\sqrt[3]{R}b}+1\right)-\frac{m_1'\sqrt[3]{n}}{\sqrt[3]{R}b}\\[0.4em]
   +\left( \sqrt[3]{n}\sqrt[3]{R}^2 \text{ mod }\floor*{\frac{m\sqrt[3]{n}}{\sqrt[3]{R}b}}\right)\
  & \text{\hspace*{-0.9em}if } \sqrt[3]{\frac{R}{n}} \leq \frac{m}{b} < \sqrt[3]{R}\\[0.4em]

  \ceil*{\frac{Rb}{m_2'}}\left(\sqrt[3]{n}+1\right)\
  & \text{\hspace*{-0.9em}if } \sqrt[3]{R} \leq \frac{m}{b} < \frac{\sqrt[3]{R}^2}{\sqrt[3]{n}}\\[0.4em]

  \ceil*{\frac{Rb}{m_3'}}\left(\frac{m'_3\sqrt[3]{n}^2}{\sqrt[3]{R}^2b}+1\right)-\frac{m'_3\sqrt[3]{n}^2}{\sqrt[3]{R}^2b}\\[0.4em]
  +\sqrt[3]{n}\left( \sqrt[3]{nR} \text{ mod } \floor*{\frac{m\sqrt[3]{n}}{\sqrt[3]{R}^2b}} \right)
  & \text{\hspace*{-0.9em}if } \frac{\sqrt[3]{R}^2}{\sqrt[3]{n}} \leq \frac{m}{b} < \sqrt[3]{R}^2\\[0.4em]

  \ceil*{\frac{Rb}{m_4'}}\left(\sqrt[3]{n}^2+1\right)\
  & \text{\hspace*{-0.9em}if } \sqrt[3]{R}^2 \leq \frac{m}{b} < \frac{R}{\sqrt[3]{n}}\\[0.4em]

  \ceil*{\frac{Rb}{m_5'}}\left(\frac{m_5'n}{Rb}+1\right)-\frac{m_5'n}{Rb}\\[0.4em]
  +\sqrt[3]{n}^2\left( \sqrt[3]{n} \text{ mod } \floor*{\frac{m\sqrt[3]{n}}{Rb}}\right)
  & \text{\hspace*{-0.9em}if } \frac{R}{\sqrt[3]{n}} \leq \frac{m}{b} < R
\end{cases} \label{eq:seeks-multiple-reads}
\end{equation}


\subsection{Lossless compression}

Lossless compression might reduce overall read and write times by
reducing seek time and data sizes. However, it needs to be implemented
on-the-fly, i.e., in chunks. See the work in~\cite{rajna2015speeding}
and
\href{https://github.com/pauldmccarthy/indexed\_gzip}{corresponding
  implementation}. This is straightforward to apply for random reads
but not for random writes. Multiple reads is the only merging strategy
that works with literally no seek in the reconstructed image which
could therefore potentially use in-memory compression.

\todo{could we merge without de-compressing?}

\subsection{Implementation}
\label{sec:implementation}

Our algorithms are implemented in Python using the Nibabel
library. \todo{Val\'erie and Yongping: (1) extract ImageUtils from
  mapreduce repository (preserve git history; just copy the repo and
  remove the mapreduce stuff in the copy). Link to the github
  repo. (2) Describe the code briefly, in particular the
  implementation tricks used for each implementation.}

\begin{itemize}
\item In MR, no seeking withing the blocks.
\item Avoid in-memory copies.
\end{itemize}

\subsubsection{Cluster reads}

\subsubsection{Multiple reads}

For faster reading all the blocks, before the merging process start,
we read all the split’s headers to get their position information and
calculate each row’s index in the reconstructed image. We only read the
split when there is a row in that writing range (available memory),
otherwise, we skip it.

Initially, we use a numpy array as our data buffer, after the split is read,
we assign the rows (a numpy array of unsigned integers) into this
buffer to the right position. However, after some tests, we found that
there is some overhead. Each time we assign the data to the corresponding
position of the buffer, the system will copy these integers first.
This is neither time-efficient nor space-efficient.

We then think of a better solution which we use the dictionary in python
as our data buffer. We use the row’s index as a key, the data (in bytes)
as the value. As for the multiple reads, the data buffer only contains
contiguous bytes, we can just seek once (the first key of the sorted keys
in dictionary), then write all the bytes continuously to the file.
There is no copy in the memory, which proves a bit faster than using
a numpy array as the buffer.

\todo{Yongping: among other implementation techniques (e.g. dictionary
  vs list, etc), explain clearly and concisely what the calculation
  time covers.}

\section{Simulations}

\todo{Make more simulations to support that claim.}

After running a few simulations, we saw that the comparison between
Cluster reads and Multiple reads essentially depends on the three
Cluster reads configurations.  When Cluster reads merge incomplete
block rows (case 1), they are always outperformed by Multiple reads.
When Cluster reads merge complete block rows (case 2), the comparison
between Cluster reads and Multiple reads depends on the values of $D$
and $n$. Figure~\ref{fig:model-comparison} shows two situations where
Cluster reads and Multiple reads are alternatively better than each
other. When Cluster reads merge complete block slices (case 3),
Cluster reads and Multiple reads are equivalent.

\begin{figure}
  \includegraphics[width=0.45\columnwidth]{figures/model-big-brain-creads.pdf}
  \includegraphics[width=0.45\columnwidth]{figures/model-big-brain-mreads.pdf}
  \caption{Number of seeks for Cluster reads (Left) and Multiple reads (Right), for
    D=3000, b=2 and n=125.}
  % Gnuplot file: scripts/model/model.gnplt
  \label{fig:model-individual}
\end{figure}


\begin{figure}
  \includegraphics[width=0.45\columnwidth]{figures/model-big-brain-comparison.pdf}
  \includegraphics[width=0.45\columnwidth]{figures/model-big-brain-comparison-moreblocks.pdf}
  \caption{Number of seeks for Cluster reads vs Multiple reads when
    Cluster reads merge complete block rows (case 2), for D=3000 and
    b=2. Left: n=125; Right: n=64,000.}
  % Gnuplot file: scripts/model/model.gnplt
  \label{fig:model-comparison}
\end{figure}

\section{Benchmark}
\label{sec:benchmark}

\subsection{Data}
To benchmark our library we use the 76~GB Big~Brain image split in 125
non-overlapping chunks. The data is available \todo{[here]}.

\subsection{Infrastructure}

We run experiments using a Dell Precision Tower 3620 with CentOS Linux release
7.3.1611, 32~GB of RAM and two disks:
\begin{itemize}
  \item Hard disk drive (HDD): HGST Travelstar 7K1000, 7200~rpm, 931GiB (1TB), firmware version JB0OA3W0.
    \item Solid-state drive (SSD): SanDisk X400 2.5, 238GiB (256GB),
      firmware version X4130012.
\end{itemize}
Both drives used 512-byte logical sectors, 4096-byte physical sectors,
SATA >3.1 (6.0 Gb/s) and were accessed through the XFS file system
v4.5.0.
%smartctl --xall /dev/sda


\section{Experiments}
\label{sec:results}

\subsection{Execution conditions}

We used release xxx of our library. 

Experiment scripts are available at xxx.
As our experiments results are heavily rely on IO performance of the disk,
we drop the system cache to get more precise result before each experiment.

\subsection{Slices vs Blocks}

Figure~\ref{fig:slices-vs-blocks} shows the merging time for Slices vs
Blocks, on SSD and HDD. Merging with Blocks takes about twice the time
of Slices, which motivates the need for improved algorithms to merge
blocks. As expected, seek time is mainly responsible for this
difference, although differences in read times are also observed,
perhaps due to a caching effect at the disk or OS level. This behavior is
reproduced in SSD and HDD, which shows that SSDs might not be the only
solution to reduce seek time.  \todo{On figure: check that breakdowns
  add up to total time. Harmonize time formats. Harmonize colors (all
  slices in purple, all blocks in green). If time permits, try
  histogram for breakdown, on same figure than total time.}
\begin{figure}[h]
  \centering
  \includegraphics[width=0.45\columnwidth]{figures/blocks-slices/totalmergetimeSSD1.pdf}
  \hfill
  \includegraphics[width=0.45\columnwidth]{figures/blocks-slices/totalmergetimeHDD1.pdf}\\
  \includegraphics[width=0.45\columnwidth]{figures/blocks-slices/hddBreakDown1.pdf}
  \hfill
  \includegraphics[width=0.45\columnwidth]{figures/blocks-slices/ssdBreakDown1.pdf}
  \caption{Merging time for Slices and Blocks. Left column: SSD. Right column: HDD. Top row: overall merging time. Bottom row: breakdown by read, write and seek time.}
  % Gnuplot file: scripts/blocks-slices/createboxplot.gnplt
\label{fig:slices-vs-blocks}
\end{figure}

\subsection{Buffered slices}

\subsection{Cluster reads}

\subsection{Multiple reads}

\begin{figure}[h]
  \centering
  \includegraphics[width=0.45\columnwidth]{figures/benchmark-mreads/mreads-breakdown-ssd.pdf}
  \hfill
    \includegraphics[width=0.45\columnwidth]{figures/benchmark-mreads/mreads-breakdown-hdd.pdf}
  \caption{Comparison between Multiple reads, blocks and slices. Left: SSD. Right: HDD.}
  % Python file: scripts/mreads/generate_avg_var_data.py
  % Gnuplot file: scripts/mreads/compare_breakdown_errbar.gnuplot
\label{fig:multiple-reads}
\end{figure}

\begin{figure}[h]
  \centering
  \includegraphics[width=0.70\columnwidth]{figures/benchmark-mreads/mreads-number-of-seeks.pdf}
  \hfill
  \caption{Number of seeks in Multiple reads}
  % Gnuplot file: scripts/mreads/number_of_seeks.gnuplot
\label{fig:multiple-reads-number-of-seek}
\end{figure}


Results discussion:
\begin{itemize}
\item Comment on calculation time.
\item Write time is higher than read time because use nibabel for writing but not reading.
\item It seems that the merge time is quite stable w.r.t the amount of available memory. This is good news for multiple reads. 
\end{itemize}

\subsection{Cluster reads vs Multiple reads}

\subsection{Lossless Compression}

\subsection{Splitting}

\subsubsection{Multiple writes}
\begin{figure}[h]
  \centering
  \includegraphics[width=0.45\columnwidth]{figures/benchmark-mwrites/mwrites-breakdown-ssd.pdf}
  \hfill
    \includegraphics[width=0.45\columnwidth]{figures/benchmark-mwrites/mwrites-breakdown-hdd.pdf}
  \caption{Comparison between Multiple writes, blocks and slices. Left: SSD. Right: HDD.}
  % Gnuplot file: scripts/blocks-slices/createboxplot.gnplt
\label{fig:multiple-writes}
\end{figure}


\newpage

\section{Discussion}
\label{sec:discussion}

% Problem solved!
Cluster reads and multiple reads are able to reduce to a negligible
amount the overall seek time required to split or merge 3D blocks in a
high-resolution image where data is stored linearly. Both algorithms
performed equivalently on our benchmark and compared to the reference
configuration where slices are merged or split without seeking. Our
initial problem is solved.

% Parallel algorithms
In a real system, high-resolution images are likely to be processed on
computing clusters, for instance using software from the Hadoop
project, in particular the Hadoop Distributed File
System~\cite{shvachko2010hadoop}. In this context parallel split-and-merge algorithms
will be required, since the various blocks of a large image would be
uploaded to different disks concurrently. In the same vein,
``re-spliting'' algorithms would have to be devised in case a split
image need to be split in a different geometry. Cluster reads and
multiple reads could be used as starting points for such algorithms.

% Several other factors might influence performance
I/O optimization is a holistic problem that is in practice highly
dependent on the hardware used, firmware, operating system, file
system and programming language. Caching occurs at various levels and
might always influence performance, potentially differently depending
on the split or merge algorithm used. In some disks, seek time greatly
varies with the seek distance, which would open the door to additional
opportunities for I/O optimization. Interactions between those
components might also result in performance differences particular to
a particular algorithm. To ensure the portability of our library
across systems and configurations, we focused on reducing the overall
number of seeks and ignored specific system configurations. We
demonstrated the performance of our methods on both an HDD and an SSD
disk, using state-of-the-art and widely used versions of Linux
(CentOS7) and file systems (XFS v4.5.0).

% File formats
In terms of file formats, our results demonstrate that simple imaging
formats might be used for the split-and-merge problem without
performance loss compared to more complex formats that try to preserve
spatial locality on disk, for instance MINC 2.0~\cite{vincent2016minc}
or the format based on space-filling curves mentioned
in~\cite{burns2013open}. This is of major interest in the current
open-science context since simpler formats favor data-sharing and
interoperability.  Moreover, our algorithms could potentially be
adapted to any split geometry, even though we demonstrated them on
slices and blocks only, while file formats inevitably assume a
particular geometry. For instance, the format in~\cite{burns2013open}
wouldn't perform nicely with a naive splitting algorithm if slices had
to be extracted instead of blocks.  Two points need to be made to be
fair though. First, we aimed at extracting all the blocks while the
problem targeted by the format in~\cite{burns2013open} was to extract
a single block from a large image. Second, we have not considered
on-the-fly data compression yet, which is a unique feature of MINC
2.0.

% Library is available. We recommend using xxx for most cases. Same
% algo works for blocks or slices.

% Ideas for the future: measure the actual amount of consumed memory.

\section*{Acknowledgment}

(If not added as co-authors): Greg Kiar, Pierre Bellec, Claude Lepage, Lindsay B. Lewis.

\bibliographystyle{IEEEtran}
\bibliography{IEEEabrv,biblio.bib}

\end{document}
